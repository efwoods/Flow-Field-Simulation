{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743c0ceb",
   "metadata": {},
   "source": [
    "# Ocean Flow Data Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e286f",
   "metadata": {},
   "source": [
    "## Imports, Settings, Function Definitions, and Loading Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d039f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.multiprocessing as mp\n",
    "from itertools import product\n",
    "# Suppress only the ConvergenceWarning\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(\"Using device:\", device)\n",
    "torch.set_default_device(device=device)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Global Variables\n",
    "DATA_DIR = \"OceanFlow/\"\n",
    "NUM_TIMESTEPS = 100 # T = 1 to 100\n",
    "GRID_SPACING_KM = 3\n",
    "TIME_SPACING_H = 3\n",
    "PAUSE_BETWEEN_FRAMES = 100 # milliseconds\n",
    "\n",
    "# Import custom functions\n",
    "from ocean_flow_utility import (\n",
    "    squared_exponential_kernel, \n",
    "    euclidean_distance, \n",
    "    select_distant_indices, \n",
    "    compute_magnitude, \n",
    "    load_flow_data, \n",
    "    plot_vector_field, \n",
    "    plot_single_frame, \n",
    "    compute_conditional_mean_variance_fixed_hyperparams, \n",
    "    causal_moving_average,\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep, \n",
    "    plot_particle_debris_movement, \n",
    "    kernel_multi_dimension, \n",
    "    create_partitions, \n",
    "    compute_log_likelihood, \n",
    "    logpdf_normal, \n",
    "    simulate_expanded_particle_movement, \n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days, \n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days_monitoring_stations\n",
    ")\n",
    "\n",
    "from ocean_flow_parallel import (\n",
    "    compute_log_likelihood, \n",
    "    rbf_kernel, \n",
    "    compute_conditional, \n",
    "    compute_ll_for_var_lengthscale_of_U_and_V_component, \n",
    "    predict_conditional_mean_variance_fixed_hyperparams, \n",
    "    find_future_magnitude_given_coordinates_in_kilometers, \n",
    "    simulate_expanded_particle_debris_scattering, \n",
    "    simulate_expanded_imputed_particle_debris_scattering, \n",
    "    remove_land_coordinates_and_beach_debris_X_imputed_y_imputed\n",
    ")\n",
    "\n",
    "import em\n",
    "import common\n",
    "import imageio\n",
    "\n",
    "\n",
    "# from ocean_flow_utility import *\n",
    "\n",
    "# Load the first file to determine shape\n",
    "sample_u = np.loadtxt(os.path.join(DATA_DIR, \"1u.csv\"), delimiter=\",\")\n",
    "ny, nx = sample_u.shape\n",
    "\n",
    "# Initialize 3D arrays\n",
    "U = np.zeros((NUM_TIMESTEPS, ny, nx))\n",
    "V = np.zeros((NUM_TIMESTEPS, ny, nx))\n",
    "\n",
    "# Load each timestep into the 3D array\n",
    "for t in range(1, NUM_TIMESTEPS + 1):\n",
    "    u_path = os.path.join(DATA_DIR, f\"{t}u.csv\")\n",
    "    v_path = os.path.join(DATA_DIR, f\"{t}v.csv\")\n",
    "    U[t-1] = np.loadtxt(u_path, delimiter=\",\")\n",
    "    V[t-1] = np.loadtxt(v_path, delimiter=\",\")\n",
    "\n",
    "U = torch.from_numpy(U).to(device)\n",
    "V = torch.from_numpy(V).to(device)\n",
    "U = U.to(device)\n",
    "V = V.to(device)\n",
    "MAG = (U**2 + V**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42e269",
   "metadata": {},
   "source": [
    "## HW Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About the data\n",
    "#\n",
    "# Time:\n",
    "# These are flow vectors for time T 1 to 100.\n",
    "# Thus, for example, 1u.csv gives data at a time coordinate of 0 hours. \n",
    "#\n",
    "# Depth:\n",
    "# This is the average flow from a depth near the surface to the shallower of either the bottom or 400m.\n",
    "# \n",
    "# U component:\n",
    "# *u.csv contains the horizontal components of the vectors.\n",
    "#\n",
    "# V Component: \n",
    "# *v.csv contains the vertical components of the vectors.\n",
    "#\n",
    "# Spacing:\n",
    "# The grid spacing is 3 km.\n",
    "# The matrix index (0, 0) is the bottom left of the plot.\n",
    "#\n",
    "# X-axis:\n",
    "# The columns of the u & v component csv represent the x-axis & correspond to the horizontal direction.\n",
    "#\n",
    "# Y-axis:\n",
    "# The rows of the u & v component csv represent the y-axis & correspond to the vertical direction.\n",
    "#\n",
    "# The magnitude is represented by the square-root of the squares of the sum of the squares of the u and v components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd21850",
   "metadata": {},
   "source": [
    "### Loading the data into a 3D array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data into a 3D array format, you want to construct two arrays:\n",
    "#     U[t, y, x] for horizontal flow components over time\n",
    "#     V[t, y, x] for vertical flow components over time\n",
    "# This means:\n",
    "#     The first axis = time steps (from 1 to 100)\n",
    "#     The second axis = y direction (rows in the CSV)\n",
    "#     The third axis = x direction (columns in the CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf51fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the first file to determine shape\n",
    "# sample_u = np.loadtxt(os.path.join(DATA_DIR, \"1u.csv\"), delimiter=\",\")\n",
    "# ny, nx = sample_u.shape\n",
    "\n",
    "# # Initialize 3D arrays\n",
    "# U = np.zeros((NUM_TIMESTEPS, ny, nx))\n",
    "# V = np.zeros((NUM_TIMESTEPS, ny, nx))\n",
    "\n",
    "# # Load each timestep into the 3D array\n",
    "# for t in range(1, NUM_TIMESTEPS + 1):\n",
    "#     u_path = os.path.join(DATA_DIR, f\"{t}u.csv\")\n",
    "#     v_path = os.path.join(DATA_DIR, f\"{t}v.csv\")\n",
    "#     U[t-1] = np.loadtxt(u_path, delimiter=\",\")\n",
    "#     V[t-1] = np.loadtxt(v_path, delimiter=\",\")\n",
    "\n",
    "# MAG = np.sqrt(U**2 + V**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAG = np.sqrt(U**2 + V**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAG_VAR = np.var(MAG, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_var = np.min(MAG_VAR[MAG_VAR != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, x = np.where(MAG_VAR == min_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[0] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[0] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe521f",
   "metadata": {},
   "source": [
    "### Maximum x-velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a28df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_velocity = np.max(U)\n",
    "max_index = np.unravel_index(np.argmax(U), U.shape)\n",
    "t_max, y_max, x_max = max_index\n",
    "\n",
    "print(f\"Max x-axis velocity: {max_velocity}\")\n",
    "print(f\"Location: hour={(t_max)*3}, y km={y_max*3}, x km ={x_max*3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fb210",
   "metadata": {},
   "source": [
    "### Problem 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_velocity = np.mean(MAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9eb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e260e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b2b09",
   "metadata": {},
   "source": [
    "### Identifying-water Only Locations & Correlations of long-range distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07792f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first file to determine shape\n",
    "sample_u = np.loadtxt(os.path.join(DATA_DIR, \"1u.csv\"), delimiter=\",\")\n",
    "ny, nx = sample_u.shape\n",
    "\n",
    "# Initialize 3D arrays\n",
    "U = np.zeros((NUM_TIMESTEPS, ny, nx))\n",
    "V = np.zeros((NUM_TIMESTEPS, ny, nx))\n",
    "\n",
    "# Load each timestep into the 3D array\n",
    "for t in range(1, NUM_TIMESTEPS + 1):\n",
    "    u_path = os.path.join(DATA_DIR, f\"{t}u.csv\")\n",
    "    v_path = os.path.join(DATA_DIR, f\"{t}v.csv\")\n",
    "    U[t-1] = np.loadtxt(u_path, delimiter=\",\")\n",
    "    V[t-1] = np.loadtxt(v_path, delimiter=\",\")\n",
    "\n",
    "mask = pd.read_csv('OceanFlow/mask.csv', header=None).values  # Shape: (504, 555)\n",
    "# The mask is upside-down\n",
    "mask = np.flipud(mask)\n",
    "\n",
    "INDEX = 0\n",
    "plt.imshow(U[INDEX, :, :],origin=\"lower\", cmap=\"viridis\", extent=[0, U[INDEX, :, :].shape[1], 0, U[INDEX, :, :].shape[0]], aspect='equal')\n",
    "# plt.imshow(mask_flipped, origin=\"lower\", cmap=\"viridis\", extent=[0, mask_flipped.shape[1], 0, mask_flipped.shape[0]], aspect='equal')\n",
    "\n",
    "# Plot the land over the water at time 0 \n",
    "\n",
    "# Assume U and mask are already loaded\n",
    "# U.shape = (100, 504, 555)\n",
    "# mask.shape = (504, 555)\n",
    "\n",
    "U_t_0 = U[0, :, :]  # time = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Step 1: Display the U field\n",
    "cax = ax.imshow(U_t_0,\n",
    "                origin='lower',  # (0,0) bottom-left\n",
    "                cmap='viridis',\n",
    "                extent=[0, U_t_0.shape[1], 0, U_t_0.shape[0]],\n",
    "                aspect='equal')\n",
    "\n",
    "# Step 2: Overlay the mask\n",
    "# Create a transparent mask: land is semi-transparent, water is fully transparent\n",
    "mask_display = np.ma.masked_where(mask == 1, mask)  # mask out water, show land\n",
    "ax.imshow(mask_display,\n",
    "          origin='lower',\n",
    "          cmap='gray',    # land will appear gray\n",
    "          alpha=0.3,      # transparency (0=transparent, 1=opaque)\n",
    "          extent=[0, U_t_0.shape[1], 0, U_t_0.shape[0]],\n",
    "          aspect='equal')\n",
    "\n",
    "# Step 3: Add colorbar and labels\n",
    "fig.colorbar(cax, ax=ax, label='U[0, y, x] value')\n",
    "\n",
    "ax.set_xlabel('x coordinate')\n",
    "ax.set_ylabel('y coordinate')\n",
    "ax.set_title('U[0, y, x] at Time = 0 with Land Mask Overlay')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the locations of the water on the map\n",
    "\n",
    "# Assume U and mask are already loaded\n",
    "# U.shape = (100, 504, 555)\n",
    "# mask.shape = (504, 555)\n",
    "\n",
    "# Step 1: Find water locations (mask == 1)\n",
    "water_locations = np.argwhere(mask == 1)  # shape (n_water_points, 2)\n",
    "y_coords, x_coords = water_locations[:, 0], water_locations[:, 1]\n",
    "\n",
    "# Step 2: Plot U[0, :, :]\n",
    "U_t_0 = U[0, :, :]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Display the grid\n",
    "cax = ax.imshow(U_t_0, \n",
    "                origin='lower',  # (0,0) at bottom-left\n",
    "                cmap='viridis',  # colormap\n",
    "                extent=[0, U_t_0.shape[1], 0, U_t_0.shape[0]],  # [x_min, x_max, y_min, y_max]\n",
    "                aspect='equal')  # square pixels\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(cax, ax=ax, label='U[0, y, x] value')\n",
    "\n",
    "# Step 3: Overlay water points\n",
    "ax.scatter(x_coords, y_coords, \n",
    "           color='cyan', \n",
    "           s=1,  # small points\n",
    "           label='Water points')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('x coordinate')\n",
    "ax.set_ylabel('y coordinate')\n",
    "ax.set_title('U[0, y, x] at Time = 0 with Water Points')\n",
    "\n",
    "# Optional: Add legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Assume U and mask are already loaded\n",
    "# U.shape = (100, 504, 555)\n",
    "# mask.shape = (504, 555)\n",
    "\n",
    "# Step 1: Reshape U to (y*x, time)\n",
    "U_reshape = U.transpose(1, 2, 0).reshape(504*555, 100)\n",
    "V_reshape = V.transpose(1, 2, 0).reshape(504*555, 100)\n",
    "\n",
    "# Step 2: Flatten the mask (y,x) -> (y*x,)\n",
    "mask_flat = mask.flatten()  # shape (504*555,)\n",
    "\n",
    "# Step 3: Keep only rows where mask == 1 (i.e., water)\n",
    "water_indices = np.where(mask_flat == 1)[0]\n",
    "\n",
    "U_water_data = U_reshape[water_indices]  # shape (n_water_points, 100)\n",
    "V_water_data = V_reshape[water_indices]\n",
    "# Step 4: Find corresponding (y, x) coordinates\n",
    "y_coords = water_indices // 555\n",
    "x_coords = water_indices % 555\n",
    "\n",
    "# Step 5: Build the coordinate matrix\n",
    "coords = list(zip(y_coords, x_coords))\n",
    "# coords_matrix = np.stack([\n",
    "#     np.tile(y_coords[:, np.newaxis], (1, 100)),\n",
    "#     np.tile(x_coords[:, np.newaxis], (1, 100))\n",
    "# ], axis=-1)  # shape (n_water_points, 100, 2)\n",
    "\n",
    "print(\"Water-only data shape:\", U_water_data.shape)\n",
    "print(\"Coordinates matrix shape:\", len(coords))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 6: Remove water points where U and V are all zeros across time\n",
    "# U_water_data: (n_water_points, 100)\n",
    "\n",
    "# Check where all values are exactly zero across all timesteps\n",
    "nonzero_mask = ~(np.all(U_water_data == 0, axis=1) | np.all(V_water_data == 0, axis=1))\n",
    "\n",
    "# Apply mask to U, V, and coords\n",
    "U_water_data = U_water_data[nonzero_mask]\n",
    "V_water_data = V_water_data[nonzero_mask]\n",
    "coords = [c for i, c in enumerate(coords) if nonzero_mask[i]]\n",
    "\n",
    "print(\"Filtered Water-only data shape:\", U_water_data.shape)\n",
    "print(\"Filtered Coordinates length:\", len(coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c50a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_water_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838143d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15546b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given:\n",
    "# - U_water_data.shape = (n_water_points, 100)\n",
    "# - coords = list of (y, x) tuples, len(coords) == n_water_points\n",
    "# - You want to reinsert U_water_data into a full U_full_reconstructed of shape (100, 504, 555)\n",
    "\n",
    "# Initialize a new full-size U matrix with zeros\n",
    "U_reconstructed = np.zeros((100, 504, 555))\n",
    "\n",
    "# Insert water data into corresponding coordinates\n",
    "for idx, (y, x) in enumerate(coords):\n",
    "    U_reconstructed[:, y, x] = U_water_data[idx]\n",
    "\n",
    "# Now U_reconstructed is fully rebuilt: land points = 0, water points = correctly restored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_reconstructed_reshape = U_reconstructed.transpose(1, 2, 0).reshape(504*555, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca834d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_reconstructed_reshape_0 = U_reconstructed_reshape[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a21e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof of reconstructed matrix\n",
    "\n",
    "# Step 2: Plot U[0, :, :]\n",
    "U_t_0 = U_reconstructed[0, :, :]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Display the grid\n",
    "cax = ax.imshow(U_t_0, \n",
    "                origin='lower',  # (0,0) at bottom-left\n",
    "                cmap='viridis',  # colormap\n",
    "                extent=[0, U_t_0.shape[1], 0, U_t_0.shape[0]],  # [x_min, x_max, y_min, y_max]\n",
    "                aspect='equal')  # square pixels\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(cax, ax=ax, label='U[0, y, x] value')\n",
    "\n",
    "# Step 3: Overlay water points\n",
    "ax.scatter(x_coords, y_coords, \n",
    "           color='cyan', \n",
    "           s=1,  # small points\n",
    "           label='Water points')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('x coordinate')\n",
    "ax.set_ylabel('y coordinate')\n",
    "ax.set_title('U[0, y, x] at Time = 0 with Water Points')\n",
    "\n",
    "# Optional: Add legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6424a41",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    " - Flatten the matrix to shape (num_water_rows, time_steps)\n",
    " - perform PCA on the time dimension to summarize the variance of each row of water\n",
    " - identify 10 random points that are greater than an arbitrary distance apart\n",
    " - compute the sparse correlation matrix for these ten points\n",
    " - perform collaborative filtering to predict the remaining correlations of the dimensionality reduced flow regions\n",
    " - Identify regions that are at least 80% correlated\n",
    " - Compute the true correlations of those regions\n",
    " - If two regions have high correlation, explain the results otherwise iterate on this algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_water_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_water_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc18b8",
   "metadata": {},
   "source": [
    "### Reducing the dimensionality of the U_water_data matrix with respect to time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df36568",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_svd, S_svd, Vt_svd = np.linalg.svd(U_water_data, full_matrices=False)\n",
    "explained_variance = np.square(S_svd)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose k to retain 99% of information\n",
    "threshold = 0.99\n",
    "k = np.argmax(cumulative_explained_variance >= threshold) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of components needed to retain {threshold*100}% information: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the matrix using the top-k components\n",
    "U_k = U_svd[: , :k]\n",
    "S_k = np.diag(S_svd[:k])\n",
    "Vt_k = Vt_svd[:k, :]\n",
    "\n",
    "U_water_data_reconstructed = U_k @ S_k @ Vt_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c842846",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_water_data_reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69954c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.80))\n",
    "])\n",
    "\n",
    "U_water_data_reduced = pca_pipeline.fit_transform(U_water_data)\n",
    "pca_model = pca_pipeline.named_steps['pca']\n",
    "print(\"Number of components selected:\", pca_model.n_components_)\n",
    "print(\"Shape of reduced data:\", U_water_data_reduced.shape)\n",
    "U_water_data_reduced_t_0 = U_water_data_reduced[:, 0]\n",
    "U_water_data_reduced_t_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4123441",
   "metadata": {},
   "source": [
    "### Identifying Long Range Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_range_indices = select_distant_indices(coords=coords, num_points=100, distance_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dadb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"long_range_distance_indices.npy\", long_range_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_range_indices = np.load(\"long_range_distance_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_water_data[long_range_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_water_data_reduced_t_0[long_range_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute correlation matrix between rows\n",
    "# n_rows = U_water_data[long_range_indices].shape[0]\n",
    "# correlation_matrix = np.zeros((n_rows, n_rows))\n",
    "\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(n_rows):\n",
    "#         correlation_matrix[i, j], _ = stats.pearsonr(U_water_data[long_range_indices][i], U_water_data[long_range_indices][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"correlation_matrix.npy\", correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.load(\"correlation_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b90ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"v_correlation_matrix.npy\", v_correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47546214",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_correlation_matrix = np.load(\"v_correlation_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute correlation matrix between rows\n",
    "# n_rows = V_water_data[long_range_indices].shape[0]\n",
    "# v_correlation_matrix = np.zeros((n_rows, n_rows))\n",
    "\n",
    "# for i in range(n_rows):\n",
    "#     for j in range(n_rows):\n",
    "#         v_correlation_matrix[i, j], _ = stats.pearsonr(V_water_data[long_range_indices][i], V_water_data[long_range_indices][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b534abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42527f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(v_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df8b30",
   "metadata": {},
   "source": [
    "### Correlation matrix above a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_correlation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_indices = np.argwhere(v_correlation_matrix > 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argwhere(correlation_matrix > 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_indices = v_indices[v_indices[:, 0] != v_indices[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef487a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = indices[indices[:, 0] != indices[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558cbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort each pair (smallest first), so (708, 38) becomes (38, 708)\n",
    "v_sorted_arr = np.sort(v_indices, axis=1)\n",
    "\n",
    "# Use a set of tuples to remove duplicates\n",
    "v_unique_pairs = np.unique(v_sorted_arr, axis=0)\n",
    "\n",
    "print(v_unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae926bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort each pair (smallest first), so (708, 38) becomes (38, 708)\n",
    "sorted_arr = np.sort(indices, axis=1)\n",
    "\n",
    "# Use a set of tuples to remove duplicates\n",
    "unique_pairs = np.unique(sorted_arr, axis=0)\n",
    "\n",
    "print(unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697bd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2baf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[long_range_indices[i] for i in unique_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972690f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[long_range_indices[i] for i in v_unique_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbb7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_correlation_values = [v_correlation_matrix[i[0], i[1]] for i in v_unique_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_values = [correlation_matrix[i[0], i[1]] for i in unique_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86224106",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a36cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_correlation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the first vertical pair of points with high correlation\n",
    "index_y_unique_pair_1_pt_0 = [long_range_indices[i] for i in v_unique_pairs[0]][0] // 504\n",
    "index_x_unique_pair_1_pt_0 = [long_range_indices[i] for i in v_unique_pairs[0]][0] % 555\n",
    "\n",
    "index_y_unique_pair_1_pt_1 = [long_range_indices[i] for i in v_unique_pairs[0]][1] // 504\n",
    "index_x_unique_pair_1_pt_1 = [long_range_indices[i] for i in v_unique_pairs[0]][1] % 555\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the first horizontal pair of points with high correlation\n",
    "index_y_unique_pair_0_pt_0 = [long_range_indices[i] for i in unique_pairs[0]][0] // 504\n",
    "index_x_unique_pair_0_pt_0 = [long_range_indices[i] for i in unique_pairs[0]][0] % 555\n",
    "\n",
    "index_y_unique_pair_0_pt_1 = [long_range_indices[i] for i in unique_pairs[0]][1] // 504\n",
    "index_x_unique_pair_0_pt_1 = [long_range_indices[i] for i in unique_pairs[0]][1] % 555\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09873cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x_unique_pair_0_pt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ed1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_y_unique_pair_0_pt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x_unique_pair_0_pt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_y_unique_pair_0_pt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_correlation_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872018ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_frame(timestep):\n",
    "    u, v = load_flow_data(timestep)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_vector_field(u, v, ax, f\"Flow Field at Time Step {timestep}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_vector_field(u, v, ax, title=\"\"):\n",
    "    ax.clear()\n",
    "    y, x = np.mgrid[0:u.shape[0], 0:u.shape[1]]\n",
    "    q = ax.quiver(x * GRID_SPACING_KM, y * GRID_SPACING_KM, u, v,\n",
    "              compute_magnitude(u, v), scale=50, cmap='viridis')\n",
    "    \n",
    "    # Add a colorbar to show magnitude scale\n",
    "    cbar = plt.colorbar(q, ax=ax)\n",
    "    cbar.set_label(\"Velocity Magnitude (km/H)\")  # Customize unit if known\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Distance X (km)\")\n",
    "    ax.set_ylabel(\"Distance Y (km)\")\n",
    "    ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the land over the water at time 0 \n",
    "\n",
    "# Assume U and mask are already loaded\n",
    "# U.shape = (100, 504, 555)\n",
    "# mask.shape = (504, 555)\n",
    "\n",
    "MAG_t_0 = MAG[0, :, :]  # time = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Step 1: Display the U field\n",
    "cax = ax.imshow(U_t_0,\n",
    "                origin='lower',  # (0,0) bottom-left\n",
    "                cmap='viridis',\n",
    "                extent=[0, U_t_0.shape[1], 0, U_t_0.shape[0]],\n",
    "                aspect='equal')\n",
    "\n",
    "# Step 2: Overlay the mask\n",
    "# Create a transparent mask: land is semi-transparent, water is fully transparent\n",
    "mask_display = np.ma.masked_where(mask == 1, mask)  # mask out water, show land\n",
    "ax.imshow(mask_display,\n",
    "          origin='lower',\n",
    "          cmap='gray',    # land will appear gray\n",
    "          alpha=0.3,      # transparency (0=transparent, 1=opaque)\n",
    "          extent=[0, U_t_0.shape[1], 0, U_t_0.shape[0]],\n",
    "          aspect='equal')\n",
    "\n",
    "# Plotting the correlated horizontal component\n",
    "plt.scatter(index_x_unique_pair_0_pt_0, index_y_unique_pair_0_pt_0, marker='x', color='red', label=f\"U component correlation: {correlation_values[0]:3f}\")\n",
    "plt.scatter(index_x_unique_pair_0_pt_1, index_y_unique_pair_0_pt_1, marker='x', color='red')\n",
    "\n",
    "# Plotting the correlated vertical component\n",
    "plt.scatter(index_x_unique_pair_1_pt_0, index_y_unique_pair_1_pt_0, marker='o', color='orange', label=f\"V component correlation: {v_correlation_values[0]:3f}\")\n",
    "plt.scatter(index_x_unique_pair_1_pt_1, index_y_unique_pair_1_pt_1, marker='o', color='orange')\n",
    "# plt.scatter(index_y_unique_pair_0_pt_1, index_x_unique_pair_0_pt_1, marker='o', color='red')\n",
    "plt.legend()\n",
    "# Step 3: Add colorbar and labels\n",
    "fig.colorbar(cax, ax=ax, label='U[0, y, x] value')\n",
    "\n",
    "ax.set_xlabel('x coordinate')\n",
    "ax.set_ylabel('y coordinate')\n",
    "ax.set_title('U[0, y, x] at Time = 0 with Land Mask Overlay')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_single_frame(timestep):\n",
    "# u, v = load_flow_data(timestep)\n",
    "# def load_flow_data(timestep):\n",
    "# u_path = os.path.join(DATA_DIR, f\"{timestep}u.csv\")\n",
    "# v_path = os.path.join(DATA_DIR, f\"{timestep}v.csv\")\n",
    "# u = np.loadtxt(u_path, delimiter=\",\")\n",
    "# v = np.loadtxt(v_path, delimiter=\",\")\n",
    "# return u, v\n",
    "U_t0 = U[0, :, :]\n",
    "V_t0 = V[0, :, :]\n",
    "fig, ax = plt.subplots()\n",
    "# plot_vector_field(u, v, ax, f\"Flow Field at Time Step {timestep}\")\n",
    "ax.clear()\n",
    "y, x = np.mgrid[0:U_t0.shape[0], 0:V_t0.shape[1]]\n",
    "q = ax.quiver(x * GRID_SPACING_KM, y * GRID_SPACING_KM, U_t0, V_t0,\n",
    "          compute_magnitude(U_t0, V_t0), scale=50, cmap='viridis')\n",
    "\n",
    "# Create a transparent mask: land is semi-transparent, water is fully transparent\n",
    "mask_display = np.ma.masked_where(mask == 1, mask)\n",
    "ax.imshow(mask_display,\n",
    "          origin='lower',\n",
    "          cmap='viridis',    # land will appear gray\n",
    "          alpha=1,      # transparency (0=transparent, 1=opaque)\n",
    "          extent=[0, U_t_0.shape[1]* GRID_SPACING_KM, 0, U_t_0.shape[0]*GRID_SPACING_KM],\n",
    "          aspect='equal')\n",
    "\n",
    "# Plotting the correlated horizontal component\n",
    "plt.scatter(index_x_unique_pair_0_pt_0* GRID_SPACING_KM, index_y_unique_pair_0_pt_0* GRID_SPACING_KM, marker='x', color='red', label=f\"U component correlation: {correlation_values[0]:3f}\")\n",
    "plt.scatter(index_x_unique_pair_0_pt_1* GRID_SPACING_KM, index_y_unique_pair_0_pt_1* GRID_SPACING_KM, marker='x', color='red')\n",
    "\n",
    "# Plot correlated horizontal component (U)\n",
    "x0_u = index_x_unique_pair_0_pt_0 * GRID_SPACING_KM\n",
    "y0_u = index_y_unique_pair_0_pt_0 * GRID_SPACING_KM\n",
    "x1_u = index_x_unique_pair_0_pt_1 * GRID_SPACING_KM\n",
    "y1_u = index_y_unique_pair_0_pt_1 * GRID_SPACING_KM\n",
    "\n",
    "# Annotate U correlation points\n",
    "ax.annotate(f\"({x0_u:.1f}, {y0_u:.1f})\", (x0_u, y0_u), textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=8, color='red')\n",
    "ax.annotate(f\"({x1_u:.1f}, {y1_u:.1f})\", (x1_u, y1_u), textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=8, color='red')\n",
    "\n",
    "# Plot correlated vertical component (V)\n",
    "x0_v = index_x_unique_pair_1_pt_0 * GRID_SPACING_KM\n",
    "y0_v = index_y_unique_pair_1_pt_0 * GRID_SPACING_KM\n",
    "x1_v = index_x_unique_pair_1_pt_1 * GRID_SPACING_KM\n",
    "y1_v = index_y_unique_pair_1_pt_1 * GRID_SPACING_KM\n",
    "\n",
    "# Plotting the correlated vertical component\n",
    "plt.scatter(index_x_unique_pair_1_pt_0* GRID_SPACING_KM, index_y_unique_pair_1_pt_0* GRID_SPACING_KM, marker='o', color='orange', label=f\"V component correlation: {v_correlation_values[0]:3f}\")\n",
    "plt.scatter(index_x_unique_pair_1_pt_1* GRID_SPACING_KM, index_y_unique_pair_1_pt_1* GRID_SPACING_KM, marker='o', color='orange')\n",
    "\n",
    "# Annotate V correlation points\n",
    "ax.annotate(f\"({x0_v:.1f}, {y0_v:.1f})\", (x0_v, y0_v), textcoords=\"offset points\", xytext=(-50,5), ha='left', fontsize=8, color='red')\n",
    "# ax.annotate(f\"({x1_v:.1f}, {y1_v:.1f})\", (x1_v, y1_v), textcoords=\"offset points\", xytext=(-15,15), ha='left', fontsize=8, color='red')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "title = f\"Magnitude of Flows at Timestep 0:\\nCorrelated Points of Horizontal and Vertical Components\"\n",
    "# Add a colorbar to show magnitude scale\n",
    "cbar = plt.colorbar(q, ax=ax)\n",
    "cbar.set_label(\"Velocity Magnitude (km/H)\")  # Customize unit if known\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"Distance X (km)\")\n",
    "ax.set_ylabel(\"Distance Y (km)\")\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x_unique_pair_0_pt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_y_unique_pair_0_pt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d77e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x_unique_pair_0_pt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_y_unique_pair_0_pt_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70488759",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First pair of correlated U-component flows:\\nCorrelation: {correlation_values[0]:3f}\\nPoint 1: x={index_y_unique_pair_0_pt_1* GRID_SPACING_KM} km, y={index_x_unique_pair_0_pt_1* GRID_SPACING_KM} km\")\n",
    "print(f\"Point 2: x={index_y_unique_pair_0_pt_0* GRID_SPACING_KM} km, y={index_x_unique_pair_0_pt_0* GRID_SPACING_KM} km\")\n",
    "\n",
    "print(f\"\\nSecond pair of correlated V-component flows:\\nCorrelation: {v_correlation_values[0]:3f}\\nPoint 1 x={index_y_unique_pair_1_pt_1* GRID_SPACING_KM} km, y={index_x_unique_pair_1_pt_1* GRID_SPACING_KM} km\")\n",
    "print(f\"Point 2 x={index_y_unique_pair_1_pt_0* GRID_SPACING_KM} km, y={index_x_unique_pair_1_pt_0* GRID_SPACING_KM} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40375776",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (index_y_unique_pair_0_pt_1* GRID_SPACING_KM, index_x_unique_pair_0_pt_1* GRID_SPACING_KM)\n",
    "p2 = (index_y_unique_pair_0_pt_0* GRID_SPACING_KM, index_x_unique_pair_0_pt_0* GRID_SPACING_KM)\n",
    "euclidean_distance(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (index_y_unique_pair_1_pt_1* GRID_SPACING_KM, index_x_unique_pair_1_pt_1* GRID_SPACING_KM)\n",
    "p2 = (index_y_unique_pair_1_pt_0* GRID_SPACING_KM, index_x_unique_pair_1_pt_0* GRID_SPACING_KM)\n",
    "euclidean_distance(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    \"\"\"Calculate Euclidean distance between two (y, x) points.\"\"\"\n",
    "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732fa2b",
   "metadata": {},
   "source": [
    "### Creating a sparse data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9694313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sparse matrix using the identified correlations; to be used for predictions\n",
    "# I have a matrix of shape (200, 200) and I want to create a matrix of (231705, 231705) based upon this sparse matrix of values. Each position of the correlation matrix corresponds to a larger matrix that is of shape (231705, 100) by using a list object of \"long_range_indices\".  I want to fill a sparse matrix of shape (231705, 231705) with the coordinates of the \"long_range_indices\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty COO sparse matrix\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "# Fill rows, cols, data\n",
    "for i in range(1200):\n",
    "    for j in range(1200):\n",
    "        corr_value = correlation_matrix[i, j]\n",
    "        mapped_i = long_range_indices[i]\n",
    "        mapped_j = long_range_indices[j]\n",
    "        \n",
    "        rows.append(mapped_i)\n",
    "        cols.append(mapped_j)\n",
    "        data.append(corr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a642e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.zeros(shape=(231705, 231705))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sparse matrix\n",
    "sparse_corr_matrix = sp.coo_matrix((data, (rows, cols)), shape=(231705, 231705))\n",
    "\n",
    "# (Optional) Convert to CSR format for faster matrix operations later\n",
    "sparse_corr_matrix = sparse_corr_matrix.tocsr()\n",
    "\n",
    "print(f\"Sparse matrix shape: {sparse_corr_matrix.shape}\")\n",
    "print(f\"Nonzero elements: {sparse_corr_matrix.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b066186",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"correlation_matrix.npy\", correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2eae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"long_range_distance_indices.npy\", long_range_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15962307",
   "metadata": {},
   "source": [
    "## Simulating particle trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5627f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAG.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe0322",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "1. Draw particle directories uniformly at random across the entire map\n",
    "2. Simulate the particle trajectories for 300 hours and provide:\n",
    "    - a plot of the initial state\n",
    "    - a plot of the final state\n",
    "    - Two plots of intermediary states\n",
    "3. Distinguish the points with a variety of colors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw particle directories uniformly at random across the entire map\n",
    "x_particle_coordinates = torch.tensor([random.randint(0, MAG[0, :, :].shape[1] -1 ) for _ in range(10)]).reshape(-1, 1) * GRID_SPACING_KM\n",
    "y_particle_coordinates = torch.tensor([random.randint(0, MAG[0, :, :].shape[0] -1 ) for _ in range(10)]).reshape(-1, 1) * GRID_SPACING_KM\n",
    "\n",
    "n_particles = y_particle_coordinates.shape[0]\n",
    "for t in range(1, 100):\n",
    "    y_new = (y_particle_coordinates[:, t-1] + V[t - 1, (y_particle_coordinates[:, t-1] / 3).int(), (x_particle_coordinates[:, t-1] / 3).int()]) # First hour of movement\n",
    "    x_new = (x_particle_coordinates[:, t-1] + U[t - 1, (y_particle_coordinates[:, t-1] / 3).int(), (x_particle_coordinates[:, t-1] / 3).int()])\n",
    "\n",
    "    y_new = (y_new + V[t - 1, (y_new / 3).int(), (x_new / 3).int()]) # Second hour of movement\n",
    "    x_new = (x_new + U[t - 1, (y_new / 3).int(), (x_new / 3).int()])\n",
    "\n",
    "    y_new = (y_new + V[t - 1, (y_new / 3).int(), (x_new / 3).int()])\n",
    "    x_new = (x_new + U[t - 1, (y_new / 3).int(), (x_new / 3).int()])\n",
    "\n",
    "    y_new = y_new.reshape(n_particles, 1) # Third hour of movement\n",
    "    x_new = x_new.reshape(n_particles, 1)\n",
    "\n",
    "    y_particle_coordinates = torch.cat([y_particle_coordinates, y_new], axis=1)\n",
    "    x_particle_coordinates = torch.cat([x_particle_coordinates, x_new], axis=1)\n",
    "\n",
    "print(f\"x_particle_coordinates.shape: {x_particle_coordinates.shape}\")\n",
    "\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep(x_particle_coordinates, y_particle_coordinates, MAG, U, V, TIMESTEP=0, ARROW_SCALE=100)\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep(x_particle_coordinates, y_particle_coordinates, MAG, U, V, TIMESTEP=16, ARROW_SCALE=100)\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep(x_particle_coordinates, y_particle_coordinates, MAG, U, V, TIMESTEP=40, ARROW_SCALE=100)\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep(x_particle_coordinates, y_particle_coordinates, MAG, U, V, TIMESTEP=99, ARROW_SCALE=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427fd75",
   "metadata": {},
   "source": [
    "## Creating a distribution of trajectories from a specific point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f618d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# Create unique colors for each particle\n",
    "num_particles = 10\n",
    "\n",
    "x_index = 100\n",
    "y_index = 350\n",
    "\n",
    "# Mean positions for each particle (could also be different)\n",
    "mean_x = 100\n",
    "mean_y = 350\n",
    "\n",
    "# Different variances for each particle\n",
    "variance_x = np.random.uniform(0, 6, size=num_particles)\n",
    "variance_y = np.random.uniform(0, 4, size=num_particles)\n",
    "\n",
    "# Standard deviations are sqrt(variance)\n",
    "std_x = np.sqrt(variance_x)\n",
    "std_y = np.sqrt(variance_y)\n",
    "\n",
    "# Sample particle positions\n",
    "sampled_x = np.random.normal(loc=mean_x, scale=std_x)\n",
    "sampled_y = np.random.normal(loc=mean_y, scale=std_y)\n",
    "\n",
    "# Object Structure: [Particle] [Time]\n",
    "sampled_y = sampled_y.astype(np.float64).reshape(-1, 1) * GRID_SPACING_KM\n",
    "sampled_x = sampled_x.astype(np.float64).reshape(-1, 1) * GRID_SPACING_KM\n",
    "sampled_y = torch.from_numpy(sampled_y).to('cuda')\n",
    "sampled_x = torch.from_numpy(sampled_x).to('cuda')\n",
    "\n",
    "\n",
    "n_particles = sampled_y.shape[0]\n",
    "\n",
    "for t in range(1, 100):\n",
    "    new_sample_y = sampled_y[:, t-1] + V[t - 1, (sampled_y[:, t-1] / 3).int(), (sampled_x[:, t-1] / 3).int()] # First Hour\n",
    "    new_sample_x = sampled_x[:, t-1] + U[t - 1, (sampled_y[:, t-1] / 3).int(), (sampled_x[:, t-1] / 3).int()]\n",
    "\n",
    "    new_sample_y = new_sample_y + V[t - 1, (new_sample_y / 3).int(), (new_sample_x / 3).int()] # Second Hour\n",
    "    new_sample_x = new_sample_x + U[t - 1, (new_sample_y / 3).int(), (new_sample_x / 3).int()] # Second Hour\n",
    "\n",
    "    new_sample_y = new_sample_y + V[t - 1, (new_sample_y / 3).int(), (new_sample_x / 3).int()] # Third Hour\n",
    "    new_sample_x = new_sample_x + U[t - 1, (new_sample_y / 3).int(), (new_sample_x / 3).int()] # Third Hour\n",
    "\n",
    "    new_sample_y = new_sample_y.reshape(n_particles, 1)\n",
    "    new_sample_x = new_sample_x.reshape(n_particles, 1)\n",
    "\n",
    "    sampled_y = torch.cat([sampled_y, new_sample_y], axis=1)\n",
    "    sampled_x = torch.cat([sampled_x, new_sample_x], axis=1)\n",
    "\n",
    "# Plotting\n",
    "plot_particle_debris_movement(sampled_x, sampled_y, MAG, U, V, TIMESTEP=0, ARROW_SCALE=10)\n",
    "plot_particle_debris_movement(sampled_x, sampled_y, MAG, U, V, TIMESTEP=16, ARROW_SCALE=10)\n",
    "plot_particle_debris_movement(sampled_x, sampled_y, MAG, U, V, TIMESTEP=24, ARROW_SCALE=10)\n",
    "plot_particle_debris_movement(sampled_x, sampled_y, MAG, U, V, TIMESTEP=40, ARROW_SCALE=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38f378",
   "metadata": {},
   "source": [
    "# Problem Set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85050eac",
   "metadata": {},
   "source": [
    "## Using a Kernel Function to predict movement of a particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAG_CPU = MAG.cpu()\n",
    "TIMESTEP = 0\n",
    "ARROW_SCALE = 20\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Display the grid\n",
    "cax = ax.imshow(MAG_CPU[TIMESTEP, :, :],\n",
    "                origin='lower', # (0, 0) at the bottom left \n",
    "                cmap = 'viridis', # colormap\n",
    "                extent=[0, MAG_CPU[TIMESTEP, :, :].shape[1] * GRID_SPACING_KM, 0, MAG_CPU[TIMESTEP, :, :].shape[0] * GRID_SPACING_KM], #[x_min, x_max, y_min, y_max]\n",
    "                aspect='equal') # square pixel aspect ratio\n",
    "\n",
    "ax.axhline(y=735, color='red')\n",
    "ax.axvline(x=105, color='red')\n",
    "\n",
    "# Add a colorbar\n",
    "fig.colorbar(cax, ax=ax, label='Magnitude of Flow (Km/H)')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('X coordinate (Km)')\n",
    "ax.set_ylabel('Y coordinate (Km)')\n",
    "ax.set_title(f\"Identifying an interesting location \\nto describe the data \\nwith a kernel function \\n at Hour {(TIMESTEP) * 3}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917283a",
   "metadata": {},
   "source": [
    "## Estimating the kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4043e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each timestep is 72 hours\n",
    "print(f\"Total time in hours: {300 * 24}\")\n",
    "print(f\"index of 360 hours: {360 / 72}\")\n",
    "\n",
    "# Coordinates in km\n",
    "x_point_init = 105\n",
    "y_point_init = 735\n",
    "\n",
    "# Coordinates in indices\n",
    "x_coordinate = torch.tensor(x_point_init // 3)\n",
    "y_coordinate = torch.tensor(y_point_init // 3)\n",
    "\n",
    "# Coodinates\n",
    "print(f\"x_coordinate, y_coordinate: {x_coordinate, y_coordinate}\")\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=False)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90baa1da",
   "metadata": {},
   "source": [
    "## Running the Squared Exponential Kernel Function for Three More Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_point_1 = 775\n",
    "y_point_1 = 50\n",
    "\n",
    "x_point_2 = 415\n",
    "y_point_2 = 360\n",
    "\n",
    "x_point_3 = 1075\n",
    "y_point_3 = 420\n",
    "\n",
    "TIMESTEP = 0\n",
    "ARROW_SCALE = 20\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Display the grid\n",
    "cax = ax.imshow(MAG_CPU[TIMESTEP, :, :],\n",
    "                origin='lower', # (0, 0) at the bottom left \n",
    "                cmap = 'viridis', # colormap\n",
    "                extent=[0, MAG_CPU[TIMESTEP, :, :].shape[1] * GRID_SPACING_KM, 0, MAG_CPU[TIMESTEP, :, :].shape[0] * GRID_SPACING_KM], #[x_min, x_max, y_min, y_max]\n",
    "                aspect='equal') # square pixel aspect ratio\n",
    "\n",
    "plt.scatter(x_point_1, y_point_1, label=f\"Point of Interest 1: ({x_point_1}, {y_point_1})\", color=\"red\", marker=\"x\")\n",
    "plt.scatter(x_point_2, y_point_2, label=f\"Point of Interest 2: ({x_point_2}, {y_point_2})\", color=\"red\", marker=\"^\")\n",
    "plt.scatter(x_point_3, y_point_3, label=f\"Point of Interest 3: ({x_point_3}, {y_point_3})\", color=\"red\", marker=\"o\")\n",
    "\n",
    "# Add a colorbar\n",
    "fig.colorbar(cax, ax=ax, label='Magnitude of Flow (Km/H)')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('X coordinate (Km)')\n",
    "ax.set_ylabel('Y coordinate (Km)')\n",
    "ax.set_title(f\"Identifying 3 locations of interest \\nto describe the data \\nwith a kernel function \\n at Hour {(TIMESTEP) * 3}\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef4de8",
   "metadata": {},
   "source": [
    "### Squared Exponential Kernel Function: Point of Interest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coodinates\n",
    "x_coordinate = torch.tensor(x_point_1 // 3)\n",
    "y_coordinate = torch.tensor(y_point_1 // 3)\n",
    "print(f\"x_coordinate, y_coordinate: {x_coordinate, y_coordinate}\")\n",
    "\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=False)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b74b0",
   "metadata": {},
   "source": [
    "### Squared Exponential Kernel Function: Point of Interest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad95068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coodinates\n",
    "x_coordinate = torch.tensor(x_point_2 // 3)\n",
    "y_coordinate = torch.tensor(y_point_2 // 3)\n",
    "print(f\"x_coordinate, y_coordinate: {x_coordinate, y_coordinate}\")\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=False)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751e4be",
   "metadata": {},
   "source": [
    "### Squared Exponential Kernel Function: Point of Interest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc571f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coodinates\n",
    "x_coordinate = torch.tensor(x_point_3 // 3)\n",
    "y_coordinate = torch.tensor(y_point_3 // 3)\n",
    "print(f\"x_coordinate, y_coordinate: {x_coordinate, y_coordinate}\")\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate,  verbose_tau=False)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0e2a9",
   "metadata": {},
   "source": [
    "## Varying the Tau Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial (not for paper)\n",
    "# Tau is 0.001: Tau is too small\n",
    "x_coordinate = torch.tensor(105 // 3)\n",
    "y_coordinate = torch.tensor(735 // 3)\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.001\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=True)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884514ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tau is 0.01\n",
    "x_coordinate = torch.tensor(105 // 3)\n",
    "y_coordinate = torch.tensor(735 // 3)\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.01\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=True)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tau is 0.1\n",
    "x_coordinate = torch.tensor(105 // 3)\n",
    "y_coordinate = torch.tensor(735 // 3)\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=True)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a06d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tau is 1.0\n",
    "x_coordinate = torch.tensor(105 // 3)\n",
    "y_coordinate = torch.tensor(735 // 3)\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 1.0\n",
    "\n",
    "# 7.2 hours, 36 hours, 72 hours, 144 hours, 288 hours\n",
    "# 7.2 hours, 1.5 days, 3 days, 6 days, 12 days\n",
    "ell = np.array([0.1, 0.5, 1, 2, 4])\n",
    "\n",
    "(\n",
    " u_best_sigma, \n",
    " u_best_lengthscale, \n",
    " u_best_log_likelihood, \n",
    " v_best_sigma, \n",
    " v_best_lengthscale, \n",
    " v_best_log_likelihood\n",
    ") = compute_ll_for_var_lengthscale_of_U_and_V_component(U, V, sigma_l, ell, tau, x_coordinate, y_coordinate, verbose_tau=True)\n",
    "\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "print(f\"u_best_log_likelihood: {u_best_log_likelihood}\")\n",
    "\n",
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"v_best_log_likelihood: {v_best_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc07a14",
   "metadata": {},
   "source": [
    "## Comparing against known standards for software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_point_1 = torch.tensor(105)\n",
    "y_point_1 = torch.tensor(735)\n",
    "\n",
    "# Coodinates\n",
    "x_coordinate = torch.tensor(x_point_1 // 3)\n",
    "y_coordinate = torch.tensor(y_point_1 // 3)\n",
    "print(f\"x_coordinate, y_coordinate: {x_coordinate, y_coordinate}\")\n",
    "\n",
    "# Kernel Parameters:\n",
    "variance = np.array([0.1, 1, 2, 3, 4, 5, 6])\n",
    "sigma_l = np.sqrt(variance)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# 0.003 hours, 0.03 hours, 0.3, 1.5 hours, 3 hours, 6 hours, 12 hours\n",
    "# 1.08 seconds, 10.8 seconds, 1.8 minutes, 18 minutes, 90 minutes, 3 hours, 6 hours, 12 hours\n",
    "ell = np.array([0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 4])\n",
    "\n",
    "# Create data partitions (k-fold-cross validation)\n",
    "U_partitions = create_partitions(U, y_coordinate, x_coordinate)\n",
    "V_partitions = create_partitions(V, y_coordinate, x_coordinate)\n",
    "\n",
    "U_result_l = []\n",
    "V_result_l = []\n",
    "\n",
    "U_ll_dict = {}\n",
    "V_ll_dict = {}\n",
    "\n",
    "for index in range(10):\n",
    "\n",
    "    X_train_U = np.array(range(U_partitions[index]['train'].shape[0])).reshape(-1, 1)\n",
    "    y_train_U = U_partitions[index]['train'].cpu().numpy()\n",
    "    X_test_U = np.array(range(U_partitions[index]['test'].shape[0])).reshape(-1, 1)\n",
    "    y_test_U = U_partitions[index]['test'].cpu().numpy()\n",
    "\n",
    "    X_train_V = np.array(range(V_partitions[index]['train'].shape[0])).reshape(-1, 1)\n",
    "    y_train_V = V_partitions[index]['train'].cpu().numpy()\n",
    "    X_test_V = np.array(range(V_partitions[index]['test'].shape[0])).reshape(-1, 1)\n",
    "    y_test_V = V_partitions[index]['test'].cpu().numpy()\n",
    "\n",
    "    # Manual Hyperparameter Search\n",
    "    for std in sigma_l:\n",
    "        for l in ell:\n",
    "            kernel = ConstantKernel(std**2) * RBF(length_scale=l)\n",
    "            U_model = GaussianProcessRegressor(kernel=kernel)\n",
    "            V_model = GaussianProcessRegressor(kernel=kernel)\n",
    "            U_model.fit(X_train_U, y_train_U)\n",
    "            V_model.fit(X_train_V, y_train_V)\n",
    "            U_ll = U_model.log_marginal_likelihood_value_\n",
    "            V_ll = V_model.log_marginal_likelihood_value_\n",
    "            if index == 0:\n",
    "                U_ll_dict[(std, l)] = U_ll\n",
    "                V_ll_dict[(std, l)] = V_ll\n",
    "            else:\n",
    "                U_ll_dict[(std, l)] += U_ll\n",
    "                V_ll_dict[(std, l)] += V_ll\n",
    "\n",
    "\n",
    "# After folds, average them:\n",
    "U_ll_dict_avg = {k: v / 10 for k, v in U_ll_dict.items()}\n",
    "V_ll_dict_avg = {k: v / 10 for k, v in V_ll_dict.items()}\n",
    "\n",
    "# Now select the best\n",
    "U_best_params = max(U_ll_dict_avg.items(), key=lambda x: x[1])\n",
    "V_best_params = max(V_ll_dict_avg.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfc953",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"v_best_sigma: {v_best_sigma}\")\n",
    "print(f\"v_best_lengthscale: {v_best_lengthscale}\")\n",
    "print(f\"u_best_sigma: {u_best_sigma}\")\n",
    "print(f\"u_best_lengthscale: {u_best_lengthscale}\")\n",
    "best_tau = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_SIGMA = 0.31622776601683794\n",
    "BEST_LENGTHSCALE = 2.0\n",
    "BEST_TAU = 0.1\n",
    "x_point_init = 105\n",
    "y_point_init = 735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cd9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the data\n",
    "U_time = U[:, y_point_init // 3, x_point_init // 3]\n",
    "V_time = V[:, y_point_init // 3, x_point_init // 3]\n",
    "\n",
    "U_three_days = torch.zeros(300, device=device, dtype=torch.float64)\n",
    "U_three_days[[index for index in range(0, 300, 3)]] = U_time\n",
    "\n",
    "V_three_days = torch.zeros(300, device=device)\n",
    "V_three_days[[index for index in range(0, 300, 3)]] = V_time\n",
    "\n",
    "# U_training_mean, U_training_variance = compute_conditional_mean_variance_fixed_hyperparams(U, u_best_sigma, u_best_lengthscale, tau=BEST_TAU, x_coord=x_point_init // 3, y_coord=y_point_init // 3)\n",
    "# V_training_mean, V_training_variance = compute_conditional_mean_variance_fixed_hyperparams(V, v_best_sigma, v_best_lengthscale, tau=BEST_TAU, x_coord=x_point_init // 3, y_coord=y_point_init // 3)\n",
    "\n",
    "U_X_test = torch.sort(\n",
    "    torch.cat([\n",
    "        torch.arange(1, 300, 3, device=device, dtype=torch.float64),\n",
    "        torch.arange(2, 300, 3, device=device, dtype=torch.float64)\n",
    "    ])\n",
    ")[0].reshape(-1, 1)\n",
    "U_X_train = torch.arange(0, 300, 3, device=device, dtype=torch.float64).reshape(-1, 1)\n",
    "U_y_train = U_time.reshape(-1, 1)\n",
    "U_y_test = causal_moving_average(U_y_train, 5)\n",
    "\n",
    "V_X_test = torch.sort(\n",
    "    torch.cat([\n",
    "        torch.arange(1, 300, 3, device=device, dtype=torch.float64),\n",
    "        torch.arange(2, 300, 3, device=device, dtype=torch.float64)\n",
    "    ])\n",
    ")[0].reshape(-1, 1)\n",
    "V_X_train = torch.arange(0, 300, 3, device=device, dtype=torch.float64).reshape(-1, 1)\n",
    "V_y_train = V_time.reshape(-1, 1)\n",
    "V_y_test = causal_moving_average(V_y_train, 5)\n",
    "\n",
    "# Making Predictions\n",
    "U_test_mean, U_test_cov = predict_conditional_mean_variance_fixed_hyperparams(U_X_train, U_X_test, U_y_train, sigma=BEST_SIGMA, ell=BEST_LENGTHSCALE, tau=BEST_TAU)\n",
    "V_test_mean, V_test_cov = predict_conditional_mean_variance_fixed_hyperparams(V_X_train, V_X_test, V_y_train, sigma=BEST_SIGMA, ell=BEST_LENGTHSCALE, tau=BEST_TAU)\n",
    "\n",
    "U_test_std = (U_test_cov)**0.5\n",
    "V_test_std = (V_test_cov)**0.5\n",
    "\n",
    "U_X_test_CPU = U_X_test.to('cpu')\n",
    "U_test_mean_CPU = U_test_mean.to('cpu')\n",
    "U_X_train_CPU = U_X_train.to('cpu')\n",
    "U_y_train_CPU = U_y_train.to('cpu')\n",
    "U_test_std_CPU = U_test_std.to('cpu')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(U_X_test_CPU, U_test_mean_CPU, 'b-', label='Predictive mean')\n",
    "plt.scatter(U_X_train_CPU, U_y_train_CPU, color='blue', label='Training data', zorder=5)\n",
    "plt.fill_between(\n",
    "    U_X_test_CPU.flatten(),\n",
    "    (U_test_mean_CPU.flatten() + (3 * U_test_std_CPU)),\n",
    "    (U_test_mean_CPU.flatten() - (3 * U_test_std_CPU)),\n",
    "    color='orange',\n",
    "    alpha=0.5,\n",
    "    label='±3σ band (99.7% CI)',\n",
    "    lw=1\n",
    ")\n",
    "plt.legend()\n",
    "plt.title('Gaussian Process Prediction with ±3σ Uncertainty Band: Horizontal (U) Component')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Horizontal Flow (Km/H)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "V_X_test_CPU = V_X_test.to('cpu')\n",
    "V_test_mean_CPU = V_test_mean.to('cpu')\n",
    "V_X_train_CPU = V_X_train.to('cpu')\n",
    "V_y_train_CPU = V_y_train.to('cpu')\n",
    "V_test_std_CPU = V_test_std.to('cpu')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(V_X_test_CPU, V_test_mean_CPU, 'b-', label='Predictive mean')\n",
    "plt.scatter(V_X_train_CPU, V_y_train_CPU, color='blue', label='Training data', zorder=5)\n",
    "plt.fill_between(\n",
    "    V_X_test_CPU.flatten(),\n",
    "    (V_test_mean_CPU.flatten() + (3 * V_test_std_CPU)),\n",
    "    (V_test_mean_CPU.flatten() - (3 * V_test_std_CPU)),\n",
    "    color='orange',\n",
    "    alpha=0.5,\n",
    "    label='±3σ band (99.7% CI)',\n",
    "    lw=1\n",
    ")\n",
    "plt.legend()\n",
    "plt.title('Gaussian Process Prediction with ±3σ Uncertainty Band: Vertical (V) Component')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Vertical Flow (Km/H)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd00ff",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ad15a",
   "metadata": {},
   "source": [
    "## Problem 6A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62040c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-altitude Debris Scattering (80 km sigma)\n",
    "n_particles = 100\n",
    "\n",
    "# Mean positions for each particle\n",
    "mean_x = 100\n",
    "mean_y = 350\n",
    "\n",
    "# Different variances for each particle\n",
    "variance_x = np.random.uniform(0, 80, size=n_particles)\n",
    "variance_y = np.random.uniform(0, 80, size=n_particles)\n",
    "\n",
    "# Standard deviations are sqrt(variance)\n",
    "std_x = np.sqrt(variance_x)\n",
    "std_y = np.sqrt(variance_y)\n",
    "\n",
    "# Sample particle positions\n",
    "sampled_x = np.random.normal(loc=mean_x, scale=std_x)\n",
    "sampled_y = np.random.normal(loc=mean_y, scale=std_y)\n",
    "\n",
    "# Object Structure: [Particle] [Time]\n",
    "sampled_y = sampled_y.astype(np.float64).reshape(-1, 1) * GRID_SPACING_KM\n",
    "sampled_x = sampled_x.astype(np.float64).reshape(-1, 1) * GRID_SPACING_KM\n",
    "sampled_y = torch.from_numpy(sampled_y).to('cuda')\n",
    "sampled_x = torch.from_numpy(sampled_x).to('cuda')\n",
    "\n",
    "x_particle_coordinates, y_particle_coordinates, U_expanded, V_expanded = simulate_expanded_particle_movement(n_particles=n_particles,U=U,V=V,MAG=MAG, x_particle_coordinates_init=sampled_x, y_particle_coordinates_init=sampled_y, sigma=BEST_SIGMA, lengthscale=BEST_LENGTHSCALE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a14056",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1, 72,]\n",
    "for day in days:\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days(\n",
    "        x_particle_coordinates, \n",
    "        y_particle_coordinates, \n",
    "        MAG, \n",
    "        U_expanded, \n",
    "        V_expanded, \n",
    "        DAY=day,\n",
    "        show_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d472f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_particle_coordinates_traces_and_trajectories_for_timestep_days(\n",
    "        x_particle_coordinates, \n",
    "        y_particle_coordinates, \n",
    "        MAG, \n",
    "        U_expanded, \n",
    "        V_expanded, \n",
    "        DAY=300, \n",
    "        search_locations=[780, 1250, 325, 1300],\n",
    "        show_legend=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1886613",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAG_CPU = MAG.cpu()\n",
    "TIMESTEP = -1\n",
    "ARROW_SCALE = 20\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Display the grid\n",
    "cax = ax.imshow(MAG_CPU[TIMESTEP, :, :],\n",
    "                origin='lower', # (0, 0) at the bottom left \n",
    "                cmap = 'viridis', # colormap\n",
    "                extent=[0, MAG_CPU[TIMESTEP, :, :].shape[1] * GRID_SPACING_KM, 0, MAG_CPU[TIMESTEP, :, :].shape[0] * GRID_SPACING_KM], #[x_min, x_max, y_min, y_max]\n",
    "                aspect='equal') # square pixel aspect ratio\n",
    "\n",
    "# Original coordinate\n",
    "ax.axhline(y=1250, color='red')\n",
    "ax.axvline(x=370, color='red')\n",
    "\n",
    "\n",
    "# New coordinate\n",
    "ax.axhline(y=1300, color='green')\n",
    "ax.axvline(x=325, color='green')\n",
    "\n",
    "\n",
    "fig.text(\n",
    "    0.1,\n",
    "    0.02,\n",
    "    \"* Trajectory vectors are projected by 72 hours for visibility\",\n",
    "    fontsize=10,\n",
    "    ha=\"left\",\n",
    ")\n",
    "\n",
    "# Add a colorbar\n",
    "fig.colorbar(cax, ax=ax, label='Magnitude of Flow (Km/H)')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('X coordinate (Km)')\n",
    "ax.set_ylabel('Y coordinate (Km)')\n",
    "ax.set_title(f\"Identifying an interesting location \\nto describe the data \\nwith a kernel function \\n at Hour {(TIMESTEP) * 3}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-Altitude Mid-Air Debris Scattering (200 km sigma)\n",
    "n_particles = 100\n",
    "# Mean positions for each particle\n",
    "mean_x = 100\n",
    "mean_y = 350\n",
    "\n",
    "# Different variances for each particle\n",
    "variance_x = np.random.uniform(0, 200, size=n_particles)\n",
    "variance_y = np.random.uniform(0, 200, size=n_particles)\n",
    "\n",
    "# Standard deviations are sqrt(variance)\n",
    "std_x = np.sqrt(variance_x)\n",
    "std_y = np.sqrt(variance_y)\n",
    "\n",
    "# Sample particle positions\n",
    "sampled_x = np.random.normal(loc=mean_x, scale=std_x)\n",
    "sampled_y = np.random.normal(loc=mean_y, scale=std_y)\n",
    "\n",
    "# Object Structure: [Particle] [Time]\n",
    "sampled_y = sampled_y.astype(np.float64).reshape(-1, 1) * GRID_SPACING_KM\n",
    "sampled_x = sampled_x.astype(np.float64).reshape(-1, 1) * GRID_SPACING_KM\n",
    "sampled_y = torch.from_numpy(sampled_y).to('cuda')\n",
    "sampled_x = torch.from_numpy(sampled_x).to('cuda')\n",
    "\n",
    "x_particle_coordinates, y_particle_coordinates, U_expanded, V_expanded = simulate_expanded_particle_movement(n_particles=n_particles,U=U,V=V, MAG=MAG, x_particle_coordinates_init=sampled_x, y_particle_coordinates_init=sampled_y, sigma=BEST_SIGMA, lengthscale=BEST_LENGTHSCALE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504610d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1, 40, 72, 120]\n",
    "for day in days:\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days(\n",
    "        x_particle_coordinates, \n",
    "        y_particle_coordinates, \n",
    "        MAG, \n",
    "        U_expanded, \n",
    "        V_expanded, \n",
    "        DAY=day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4aa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_particle_coordinates_traces_and_trajectories_for_timestep_days(\n",
    "        x_particle_coordinates, \n",
    "        y_particle_coordinates, \n",
    "        MAG, \n",
    "        U_expanded, \n",
    "        V_expanded, \n",
    "        DAY=300, \n",
    "        search_locations=[780, 1250, 325, 1300],\n",
    "        show_legend=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_particle_coordinates, y_particle_coordinates, U_expanded, V_expanded = simulate_expanded_particle_movement(10, U, V, MAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa474dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(x_particle_coordinates, 'x_particle_coordinates.pt')\n",
    "# torch.save(y_particle_coordinates, 'y_particle_coordinates.pt')\n",
    "# torch.save(U_expanded, \"U_expanded.pt\")\n",
    "# torch.save(V_expanded, \"V_expanded.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_particle_coordinates = torch.load('x_particle_coordinates.pt')\n",
    "y_particle_coordinates = torch.load('y_particle_coordinates.pt')\n",
    "U_expanded = torch.load(\"U_expanded.pt\")\n",
    "V_expanded = torch.load(\"V_expanded.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1, 40, 72, 120, 300]\n",
    "for day in days:\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days(\n",
    "        x_particle_coordinates, \n",
    "        y_particle_coordinates, \n",
    "        MAG, \n",
    "        U_expanded, \n",
    "        V_expanded, \n",
    "        DAY=day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import ocean_flow_utility\n",
    "# importlib.reload(ocean_flow_utility)\n",
    "# from ocean_flow_utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0124c8c",
   "metadata": {},
   "source": [
    "## Problem 6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686db05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_SIGMA = 0.31622776601683794\n",
    "BEST_LENGTHSCALE = 2.0\n",
    "BEST_TAU = 0.1\n",
    "x_point_init = 105\n",
    "y_point_init = 735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161993b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a79a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_particle_coordinates, y_particle_coordinates, U_expanded, V_expanded = simulate_expanded_particle_debris_scattering(\n",
    "    U, \n",
    "    V, \n",
    "    MAG, \n",
    "    n_particles=n_particles, \n",
    "    sigma=BEST_SIGMA, \n",
    "    lengthscale=BEST_LENGTHSCALE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbf725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(x_particle_coordinates, 'x_particle_coordinates_0.pt')\n",
    "# torch.save(y_particle_coordinates, 'y_particle_coordinates_0.pt')\n",
    "# torch.save(U_expanded, \"U_expanded_0.pt\")\n",
    "# torch.save(V_expanded, \"V_expanded_0.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe344ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('x_particle_coordinates.npy', x_particle_coordinates.cpu().numpy())\n",
    "# np.save('y_particle_coordinates.npy', y_particle_coordinates.cpu().numpy())\n",
    "# np.save(\"U_expanded.npy\", U_expanded.cpu().numpy())\n",
    "# np.save(\"V_expanded.npy\", V_expanded.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a94de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_particle_coordinates = np.load('x_particle_coordinates.npy')\n",
    "y_particle_coordinates = np.load('y_particle_coordinates.npy')\n",
    "U_expanded = np.load(\"U_expanded.npy\")\n",
    "V_expanded = np.load(\"V_expanded.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pd.read_csv(\"./OceanFlow/mask.csv\").to_numpy()\n",
    "# mask is upside-down\n",
    "mask = np.flipud(mask)\n",
    "# Assuming `mask` has shape (503, 555) and `coord_plane` has shape (504, 555)\n",
    "# Pad the mask at the bottom by 1 row (with zeros)\n",
    "mask = np.pad(mask, ((0, 1), (0, 0)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "# # Verify mask\n",
    "# import matplotlib.pyplot as plt\n",
    "# # \n",
    "# plt.imshow(mask, origin='lower', cmap='gray')  # origin='lower' puts (0,0) bottom-left\n",
    "# plt.title(\"Land Mask (0 = land, 1 = ocean)\")\n",
    "# plt.xlabel(\"x index\")\n",
    "# plt.ylabel(\"y index\")\n",
    "# plt.colorbar(label=\"Mask Value\")\n",
    "# plt.show()\n",
    "\n",
    "# x = np.int64(x_particle_coordinates.cpu().numpy() // GRID_SPACING_KM)\n",
    "# y = np.int64(y_particle_coordinates.cpu().numpy() // GRID_SPACING_KM)\n",
    "# for t in range(300):\n",
    "#     land_mask_t = mask[y[:, 0], x[:, 0]] == 0\n",
    "\n",
    "#     # Apply the mask to eliminate coordinates over land\n",
    "#     x_masked = np.where(land_mask_t, np.nan, x)\n",
    "#     y_masked = np.where(land_mask_t, np.nan, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ecef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed_masked_torch_km, y_imputed_masked_torch_km, U_expanded_torch, V_expanded_torch = remove_land_coordinates_and_beach_debris_X_imputed_y_imputed(x_particle_coordinates, y_particle_coordinates, U_expanded, V_expanded)\n",
    "\n",
    "# Convert Expanded arrays to tensors\n",
    "U_expanded_torch = torch.tensor(U_expanded, device=device, dtype=torch.float64)\n",
    "V_expanded_torch = torch.tensor(V_expanded, device=device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_masked_torch = torch.tensor(x_masked,device=device, dtype=torch.float64)\n",
    "# y_masked_torch = torch.tensor(y_masked,device=device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d25f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_masked_torch = x_masked_torch * GRID_SPACING_KM\n",
    "# y_masked_torch = y_masked_torch * GRID_SPACING_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f523c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_particle_coordinates = torch.load('x_particle_coordinates_0.pt')\n",
    "# y_particle_coordinates = torch.load('y_particle_coordinates_0.pt')\n",
    "# U_expanded = torch.load(\"U_expanded_0.pt\")\n",
    "# V_expanded = torch.load(\"V_expanded_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GIF\n",
    "\n",
    "plot_n = np.arange(1, 300)\n",
    "for day in plot_n:\n",
    "    plot_filename = f\"simulation_{day}.png\"\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days(X_imputed_masked_torch_km, y_imputed_masked_torch_km, MAG, U_expanded_torch, V_expanded_torch, DAY=day, ARROW_SCALE=72, show_legend=False, save_figure_filename=plot_filename)\n",
    "plot_filename = f\"simulation_{300}.png\"\n",
    "stations = [440, 320, 670, 230, 1100, 680]\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep_days_monitoring_stations(X_imputed_masked_torch_km, y_imputed_masked_torch_km, MAG, U_expanded_torch, V_expanded_torch, DAY=300, ARROW_SCALE=72, stations=stations, save_figure_filename=plot_filename)\n",
    "\n",
    "# Natural sort using numerical values extracted from filenames\n",
    "def natural_sort_key(filename):\n",
    "    match = re.search(r\"(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else float(\"inf\")\n",
    "\n",
    "# Path to your PNG files\n",
    "output_gif = \"output.gif\"\n",
    "# List and sort PNG files\n",
    "image_folder = \"plots\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(\".png\")]\n",
    "image_files.sort(key=natural_sort_key)\n",
    "images = []\n",
    "\n",
    "for file_name in image_files:\n",
    "    file_path = os.path.join(image_folder, file_name)\n",
    "    img = Image.open(file_path)\n",
    "    images.append(img)\n",
    "\n",
    "# Save to GIF\n",
    "images[0].save(\n",
    "    output_gif,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    duration=100,  # time per frame in milliseconds\n",
    "    loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1]\n",
    "for day in days:\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days(X_imputed_masked_torch_km, y_imputed_masked_torch_km, MAG, U_expanded_torch, V_expanded_torch, DAY=day, ARROW_SCALE=72, show_legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [440, 320, 670, 230, 1100, 680]\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep_days_monitoring_stations(X_imputed_masked_torch_km, y_imputed_masked_torch_km, MAG, U_expanded_torch, V_expanded_torch, DAY=300, ARROW_SCALE=72, stations=stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01cc934",
   "metadata": {},
   "source": [
    "## Matrix Completion using EM and Gaussian Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a90f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X_imputed_1200_300.npy', X_imputed)\n",
    "# np.save('y_imputed_1200_300.npy', y_imputed)\n",
    "# np.save('U_expanded_1200_300.npy', U_expanded.cpu().numpy())\n",
    "# np.save('V_expanded_1200_300.npy', V_expanded.cpu().numpy())\n",
    "# X_imputed = np.load('X_imputed.npy')\n",
    "# y_imputed = np.load('y_imputed.npy')\n",
    "# U_expanded = np.load('U_expanded.npy')\n",
    "# V_expanded = np.load('V_expanded.npy')\n",
    "# X_imputed_1200_300 = np.load('X_imputed_1200_300.npy')\n",
    "# y_imputed_1200_300 = np.load('y_imputed_1200_300.npy')\n",
    "# U_expanded_1200_300 = np.load('U_expanded_1200_300.npy')\n",
    "# V_expanded_1200_300 = np.load('V_expanded_1200_300.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4032930",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_SIGMA = 0.31622776601683794\n",
    "BEST_LENGTHSCALE = 2.0\n",
    "BEST_TAU = 0.1\n",
    "x_point_init = 105\n",
    "y_point_init = 735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed, y_imputed, U_expanded, V_expanded = simulate_expanded_imputed_particle_debris_scattering(    \n",
    "    U, \n",
    "    V, \n",
    "    MAG, \n",
    "    n_particles_seed = 200,\n",
    "    n_particles_impute = 1000, \n",
    "    sigma=BEST_SIGMA, \n",
    "    lengthscale=BEST_LENGTHSCALE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed_torch_km, y_imputed_torch_km, U_expanded, V_expanded  = remove_land_coordinates_and_beach_debris_X_imputed_y_imputed(X_imputed, y_imputed, U_expanded, V_expanded, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_expanded_torch = torch.tensor(U_expanded, device=device, dtype=torch.float64)\n",
    "V_expanded_torch = torch.tensor(V_expanded, device=device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb271908",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1, 40, 72, 120]\n",
    "for day in days:\n",
    "    plot_particle_coordinates_traces_and_trajectories_for_timestep_days(X_imputed_torch, y_imputed_torch, MAG, U_expanded_torch, V_expanded_torch, DAY=day, ARROW_SCALE=72, show_legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca658a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [650, 500, 800, 1000, 1100, 680]\n",
    "plot_particle_coordinates_traces_and_trajectories_for_timestep_days_monitoring_stations(X_imputed_torch, y_imputed_torch, MAG, U_expanded_torch, V_expanded_torch, DAY=300, ARROW_SCALE=72, stations=stations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
